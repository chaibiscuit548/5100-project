---
title: "Confidence in Education"
author: "Ari Gelbard"
format: 
  html:
    embed-resources: true
editor: visual
toc: true
---

------------------------------------------------------------------------

This document aims to explores the relationship between confidence in the US education system and actual government spending on education.

# Loading Packages and Data

In this section, we pull in data downloaded from the Bureau of Economic Analysis (BEA) website and from the General Social Survey (GSS).

```{r, message=FALSE}
### Load packages
library(tidyverse)
library(readxl)
require(forecast)
library(zoo)
library(kableExtra)
library(astsa)

### Load data
gved_y_raw <- read_excel("data/nipa_3155.xlsx")
gv_y_raw   <- read_excel("data/nipa_116a.xlsx")
gv_q_raw   <- read_excel("data/nipa_116q.xlsx")
pop_raw    <- read_excel("data/nipa_21q.xlsx")
gss_raw    <- read_excel("data/gss_educ.xlsx")
```

# Cleaning data

First, lets clean the data on government spending over time in the US. Unfortunately, the BEA's data on inflation adjusted spending on education only goes back to 2007. To work around this, we can take the ratio of nominal education spending to nominal total government spending and multiply real total government spending to get an estimate of real total government spending on education. However, total government spending on education is still misleading since it is increasing with things like population growth. To fix this, we can calculate education spending per capita by dividing by the U.S. population. We create two dataframes for the analysis: one with yearly data that we can use to compare to the yearly GSS data, and another that is quarterly for time series analysis.

```{r}
### BEA NIPA data 
## Population to get per capita spending 
pop <- pop_raw %>%
  select(-1) %>%
  slice(-(1:4)) %>%
  filter(row_number() %in% c(1,2,45)) %>%
  t() %>% as.data.frame() %>%
  slice(-1) %>%
  rename(year = V1, quarter = V2, popq = V3) %>%
  fill(year, .direction = "down") %>% 
  mutate(yearq = paste0(year,quarter),
         year  = as.numeric(year),
         popq = as.numeric(popq)) %>%
  select(year, yearq, popq)

pop_y <- pop %>%
  group_by(year) %>%
  summarise(popy = mean(popq, na.rm = TRUE))

## Getting ratio of education spending to total
govt_educ_y <- gved_y_raw %>%
  select(-1) %>%
  slice(-(1:4)) %>%
  filter(row_number() %in% c(1,3,31)) %>%
  t() %>% as.data.frame() %>%
  slice(-1) %>%
  rename(year = V1, govt = V2, educ = V3) %>%
  mutate(educ = as.numeric(educ),
         govt = as.numeric(govt),
         year = as.numeric(year)) %>%
  mutate(educ_govt = educ / govt)

## Getting annual government spending 
govt_y <- gv_y_raw %>%
  select(-1) %>%
  slice(-(1:4)) %>%
  filter(row_number() %in% c(1,24)) %>% 
  t() %>% as.data.frame() %>%
  slice(-1) %>%
  rename(year = V1, govty = V2) %>%
  mutate(year = as.numeric(year),
         govty = as.numeric(govty)) %>%
  left_join(govt_educ_y %>% select(year, educ_govt), by = "year") %>%
  filter(!is.na(educ_govt)) %>%
  mutate(gved_y = as.numeric(govty) * as.numeric(educ_govt)) %>%
  inner_join(pop_y, by = "year") %>%
  mutate(gved_pc_y = (gved_y * 1e6) / (popy * 1e3)) %>%
  select(year, gved_pc_y)
  
## Quarterly government spending on education (for time series forecast)
govt_q <- gv_q_raw %>%
  select(-1) %>%
  slice(-(1:4)) %>%
  filter(row_number() %in% c(1,2,24)) %>% 
  t() %>% as.data.frame() %>%
  slice(-1) %>%
  rename(year = V1, quarter = V2, govtq = V3) %>%
  fill(year, .direction = "down") %>% 
  mutate(yearq   = paste0(year,quarter),
         year    = as.numeric(year),
         quarter = as.numeric(substr(quarter,2,2)),
         govtq   = as.numeric(govtq)) %>%
  select(c(year,quarter,yearq,govtq)) %>%
  left_join(govt_educ_y %>% select(year, educ_govt), by = "year") %>%
  filter(!is.na(educ_govt)) %>%
  mutate(gved_q = govtq * educ_govt) %>%
  inner_join(pop, by = c("yearq","year")) %>%
  mutate(gved_pc_q = (gved_q * 1e6) / (popq * 1e3),
         yearq = as.yearqtr(yearq)) %>%
  select(c(year,quarter,yearq, gved_pc_q))
```

Regarding the variables from the GSS, we use the following surveys:

-   The *coneduc* survey asks whether the respondent has a great deal of confidence, only some confidence, or hardly any confidence in the institution of education in the US.
-   The *nateduc* survey asks if the respondent thinks the US government is spending too much, too little, or the right amount on education.

We create two dataframes, one that has the average response per year and another that has the number of responses per response per year.

```{r}
### GSS Education variables
## Taking an average of responses per year
gss_num <- gss_raw %>%
  mutate(nateduc_num = case_when(
            nateduc == "TOO LITTLE" ~ 1,
            nateduc == "ABOUT RIGHT" ~ 2,
            nateduc == "TOO MUCH" ~ 3,
            TRUE ~ NA_real_
          ),
         coneduc_num = case_when(
            coneduc == "HARDLY ANY" ~ 1,
            coneduc == "ONLY SOME" ~ 2, 
            coneduc == "A GREAT DEAL" ~ 3,
            TRUE ~ NA_real_
            ),
         year = as.numeric(year)) %>%
  group_by(year) %>% 
  summarise(mean_nateduc = mean(nateduc_num, na.rm = TRUE),
            mean_coneduc = mean(coneduc_num, na.rm = TRUE),
            n_obs = n()) %>%
  ungroup() %>%
  mutate(mean_coneduc = na.approx(mean_coneduc, year, na.rm = FALSE))

## Counting how many responses of each type per year
gss_count <- gss_raw %>%
  mutate(nateduc = ifelse(nateduc %in% c("TOO LITTLE","ABOUT RIGHT","TOO MUCH"), nateduc, NA),
         coneduc = ifelse(coneduc %in% c("A GREAT DEAL","HARDLY ANY","ONLY SOME"), coneduc, NA)) %>%
  select(-id_) %>%
  mutate(year = as.numeric(year))  %>%
  pivot_longer(
    cols = c(nateduc, coneduc),
    names_to = "question",
    values_to = "response"
  ) %>%
  group_by(year, question, response) %>%
  summarise(n = n(), .groups = "drop")
```

```{r,include=FALSE}
rm(gved_y_raw, gv_y_raw, gv_q_raw, pop_raw, pop, pop_y, govt_educ_y)
```

# Analysis

First, lets look at how government spending on education per capita has evolved over time.

```{r}
## NIPA Data
ggplot(data = govt_y, aes(x=year,y=gved_pc_y)) +
  geom_line() + 
  labs(title="Real Government Spending on Education Per Capita",
       x = "Year",
       y = "Chained (2017) Dollars") +
  theme_minimal()
```

This chart shows that the government has been spending more per capita on education over time. There were some dips in the 1980s and in the early 2010s, but overall it has been increasing. However, spending has stagnated a bit since the early 2000s.

Now that we've seen how spending has changed over time, lets look at confidence in education. We can make line charts for the proportion of each response the coneduc question.

```{r}
## GSS Data
gss_prop <- gss_count %>%
  filter(!is.na(response)) %>%
  group_by(year, question) %>%
  mutate(prop = n / sum(n)) 

# Confidence in Education
coneduc_prop <- gss_prop %>%
  filter(question == "coneduc")

ggplot(coneduc_prop, aes(x = year, y = prop, color = response)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(
    title = "How much confidence do you have in the education\nsystem in the US?",
    x = "Year",
    y = "Proportion of Respondents",
    color = "Response"
  ) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5))
```

It appears that confidence in the education system has decreased over time. The proportion of respondents who have a great deal of confidence has decreased from around 40% to less than 20% and the proportion who have hardly any confidence has increased from less than 10% to around 25%.

We can also perform a chi-squared test for independence to test if the distribution of GSS responses is different between two time periods. The hypotheses are:

$H_0:$ The distribution of responses to how much confidence in education is the same prior to 1990 vs after 1990.

$H_a:$ The distribution of responses is not the same prior to 1990 vs after 1990.

```{r}
# Choose two periods for comparison (example: early vs recent)
gss_test1 <- gss_raw %>%
  filter(coneduc %in% c("HARDLY ANY","ONLY SOME","A GREAT DEAL")) %>%
  mutate(period = ifelse(year <= 1990, "Early", "Late")) %>%
  group_by(period, coneduc) %>%
  summarise(n = n(), .groups = "drop") %>%
  pivot_wider(names_from = coneduc, values_from = n)

gss_test1

# Run the chi-square test
chisq.test(gss_test1[ , -1])   
```

Since the p-value is less than 0.05, we can reject the null hypothesis at the 5% level. In other words, we have sufficient statistical evidence to conclude that the distribution of responses is different prior to 1990 vs after 1990.

What about feelings about education spending? Let's make a similar line chart over time to explore this.

```{r}
# Spending on Education
nateduc_prop <- gss_prop %>%
  filter(question == "nateduc")

ggplot(nateduc_prop, aes(x = year, y = prop, color = response)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(
    title = "Is US spending on education too much, too little, or about right?",
    x = "Year",
    y = "Proportion of Respondents",
    color = "Response"
  ) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5))
```

Since the beginning of the survey, most respondents believed that the government was spending too little on education, and this proportion has increased significantly to almost 80% of respondents.

We can also perform a chi-squared test for independence to test if the distribution of GSS responses is different between two time periods. The hypothese are:

$H_0:$ The distribution of responses to how much are we spending on education is the same prior to 1990 vs after 1990.

$H_a:$ The distribution of responses is not the same prior to 1990 vs after 1990.

```{r}
# Choose two periods for comparison (example: early vs recent)
gss_test2 <- gss_raw %>%
  filter(nateduc %in% c("TOO LITTLE","ABOUT RIGHT","TOO MUCH")) %>%
  mutate(period = ifelse(year <= 1990, "Early", "Late")) %>%
  group_by(period, nateduc) %>%
  summarise(n = n(), .groups = "drop") %>%
  pivot_wider(names_from = nateduc, values_from = n)

gss_test2

# Run the chi-square test
chisq.test(gss_test2[ , -1])   
```

Again, since the p-value is less than 0.05, we can reject the null hypothesis at the 5% level. In other words, we have sufficient statistical evidence to conclude that the distribution of responses is different prior to 1990 vs after 1990.

Finally, lets plot the mean of these two variables over time to create an even more clear visualization of the decline in sentiment.

```{r, warning=FALSE}
gss_long <- gss_num %>%
  pivot_longer(
    cols = c(mean_nateduc, mean_coneduc),
    names_to = "variable",
    values_to = "mean_value"
  )

ggplot(gss_long, aes(x = year, y = mean_value, color = variable)) +
  geom_line(size = 1.1) +
  scale_color_manual(
    values = c(
      "mean_nateduc" = "firebrick",  
      "mean_coneduc" = "steelblue"   
    ),
    labels = c(
      "mean_nateduc" = "Mean Perception\nof Spending",
      "mean_coneduc" = "Mean Confidence\nin Education"
    )
  ) +
  labs(
    title = "Mean Education Attitudes Over Time",
    x = "Year",
    y = "Mean Value",
    color = "Variable"
  ) +
  theme_bw()
```

Now that we've looked at the variables of interest separately, lets investigate how spending and confidence in education relate to each other. The following charts show how opinions about the amount of education spending relate to actual education spending.

```{r}
nateduc_nipa <- gss_prop %>%
  filter(question == "nateduc") %>%
  left_join(govt_y, by = "year")

ggplot(nateduc_nipa, aes(x = gved_pc_y, y = prop)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~ response) +
  labs(
    title = "Confidence in amount of Education Spending vs Actual Spending",
    x = "Real Education Spending Per Capita",
    y = "Proportion of Respondents"
  ) +
  scale_y_continuous(labels = scales::percent) +
  theme_bw()
```

The negative slope for the respondents who believe the government spends about the right amount on education and the positive slope for the those who believe the government spends too little on education indicate that even when the government prioritize education more, people still believe that the government isn't spending enough. This may indicate that despite some efforts to spend more, the government still isn't spending as much on education as they should.

Doing the same with confidence in the education system gives similar results.

```{r}
coneduc_nipa <- gss_prop %>%
  filter(question == "coneduc") %>%
  left_join(govt_y, by = "year")

ggplot(coneduc_nipa, aes(x = gved_pc_y, y = prop)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap(~ response) +
  labs(
    title = "Confidence in the US Education System vs Spending",
    x = "Real Education Spending Per Capita",
    y = "Proportion of Respondents"
  ) +
  scale_y_continuous(labels = scales::percent) +
  theme_bw()
```

Finally, we can do the same thing but using the mean of the surveys.

```{r}
gss_num_govt <- gss_num %>%
  inner_join(govt_y, by = "year") %>%
  slice(-1)

# Mean_nateduc vs govt spending
ggplot(gss_num_govt, aes(x = gved_pc_y, y = mean_nateduc)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = "National Education Spending Attitudes vs Education Spending",
    x = "Govt Spending per Capita",
    y = "Mean National Education Attitude"
  ) +
  theme_minimal()

# Mean_coneduc vs govt spending
ggplot(gss_num_govt, aes(x = gved_pc_y, y = mean_coneduc)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = "Confidence in Education vs Education Spending",
    x = "Govt Spending per Capita",
    y = "Mean Confidence in Education"
  ) +
  theme_minimal()

```

All these plots serve to show that even as government spending on education increases, people still increasingly have lost trust in the education system in the US. Let's explore how these trends may evolve in the future with time series forecasts.

# ARIMA

In this section, we analyze and attempt to forecast the time series for education spending and confidence in education. We focus only on confidence in education rather than feelings about speding on education since it more closely relates to our overall topic of trust in american institutions.

## Government Spending on Education

```{r}
gv_ts <- ts(govt_q$gved_pc_q,
            start = c(min(govt_q$year),min(govt_q$quarter)),
            frequency = 4) 
```

Lets start with a classical decomposition.

```{r}
gv_ts_decomp <- decompose(gv_ts)
autoplot(gv_ts_decomp)
```

The trend component of the chart shows a clear upward trend over time that matches the data. The seasonal component is also showing clear seasonality with spikes that appear to occur at the beginning of each year, however the effect of the seasonality seems small compared to the trend. Finally, the remainder shows fluctuations around zero that aren't very uniform. It doesn't seem likely that the remainder is showing cyclical patterns like recessions since it doesn't seem to follow any particular pattern, but perhaps it reflects other outside political pressures on spending that are difficult to predict.

Let's plot the lag and autocorrelation function plots to investigate the patterns more closely.

```{r}
gglagplot(gv_ts, do.lines=FALSE, lags = 9) +
  xlab("Lags") + ylab("Yt") + 
  ggtitle("Lag Plot for education spending") + 
  theme(axis.text.x=element_text(angle=45, hjust=1))
```

The linear relationships in the lag plots suggest high serial correlation among consecutive observations.

```{r}
ggAcf(gv_ts, 50) +
  ggtitle("Autocorrelation Function for Real\nGovernment Spending on Education Per Capita") +
  theme_minimal()
```

This plot shows clear autocorrleation in the data, however the seasonality isn't as present. Lets confirm stationarity with an augmented Dickey Fuller test.

```{r}
tseries::adf.test(gv_ts)
```

Because the p-values for this test is greater than 0.05, we cannot reject the null hypothesis that these series are not stationary, confirming our suspicions from the ACF plot.

We can try to correct this with differencing.

```{r}
diff_1 <- diff(gv_ts)
ggAcf(diff_1,50) +
  ggtitle("ACF of First-Order Differenced Series") +
  theme_minimal()
```

It appears that one round of differencing may not have been sufficient since there is still a decreasing pattern every 4 lags. Let's try one more time to be safe.

```{r}
diff_2 <- diff(gv_ts, differences = 2)
ggAcf(diff_2,50) +
  ggtitle("ACF of Second-Order Differenced Series") +
  theme_minimal()
```

```{r}
ggPacf(diff_2,50) +
  ggtitle("PACF of the Differenced Series") +
  theme_minimal()
```

This looks a little bit better, perhaps we will need to test for both 1 and 2 lags in the Arima. With this, we can proceed with the ARIMA.

First lets find the optimal parameters. Based on the differenced ACF and PACF plot, I believe the correct range for q is 0-8 and for p is 0-4.

```{r,warning=FALSE}
# Define parameter ranges
p_range <- 0:4
d_range <- 1:2
q_range <- 0:9

# Calculate total combinations
n_combinations <- length(p_range) * length(d_range) * length(q_range)

# Create an empty matrix to store results
results_matrix <- matrix(NA, nrow = n_combinations, ncol = 6)

# Initialize index for matrix row
i <- 1

# Loop through combinations of ARIMA model parameters
for (d in d_range) {
  for (q in q_range) {
    for (p in p_range) {
  
      # Fit ARIMA model with specified (p,d,q)
      model <- Arima(gv_ts, order = c(p, d, q), include.drift = TRUE)
  
      # Store model parameters and AIC/BIC/AICc values in matrix
      results_matrix[i, ] <- c(p, d, q, model$aic, model$bic, model$aicc)
  
      # Increment row index
      i <- i + 1
    }
  }
}

# Convert matrix to data frame
results_df <- as.data.frame(results_matrix)
colnames(results_df) <- c("p", "d", "q", "AIC", "BIC", "AICc")

# Find the row with the lowest AIC
highlight_row <- which.min(results_df$AIC)

# Generate kable table with highlighting for the row with the lowest AIC
knitr::kable(results_df, align = 'c', caption = "Comparison of ARIMA Models") %>%
  kable_styling(full_width = FALSE, position = "center") %>%
  row_spec(highlight_row, bold = TRUE, background = "#FFFF99")  # Highlight row in yellow
```

```{r}
auto.arima(gv_ts)
```

The parameter search suggests an ARIMA(4,2,2) while the auto.arima() suggests ARIMA(1,2,1). Lets compare these to determine which one would be better.

```{r}
model_output1 <- capture.output(sarima(gv_ts,4,2,2))

# Find the line numbers dynamically based on a keyword
start_line <- grep("Coefficients", model_output1)  # Locate where coefficient details start
end_line <- length(model_output1)  # Last line of output

# Print the relevant section automatically
cat(model_output1[start_line:end_line], sep = "\n")
```

```{r}
model_output2 <- capture.output(sarima(gv_ts,1,2,1))
# Find the line numbers dynamically based on a keyword
start_line <- grep("Coefficients", model_output2)  # Locate where coefficient details start
end_line <- length(model_output2)  # Last line of output

# Print the relevant section automatically
cat(model_output2[start_line:end_line], sep = "\n")
```

Both **Residual Plots** show nearly consistent fluctuation around zero, suggesting that the residuals are nearly stationary with a constant mean and finite variance over time.

Both **Autocorrelation Functions (ACF)** of the residuals reveal very few significant autocorrelations, however the plot for the ARIMA(1,2,1) does have a few more autocorrelations than the ARIMA(4,2,2).

Both **Q-Q Plots** indicate that the residuals follow a near-normal distribution, with minor deviations at the tails, which is typical in time series data.

The **Ljung-Box Test** p-values for the ARIMA(4,2,2) remain above the 0.05 significance level, implying no significant autocorrelations left in the residuals while in the ARIMA(1,2,1), all p-values remain below the 0.05 significance level implying some significant autocorrelations left in the residuals and concluding that model improvements may necessary.

**Coefficient Significance**: The model coefficients in the ARIMA(1,2,1) model are all significant while the coefficients for the first three AR terms and the last MA term are insignificant in the ARIMA(4,2,2) model.

```{r}
fit1 <- Arima(gv_ts, order = c(4,2,2), include.drift = FALSE)
summary(fit1)
```

```{r}
fit2 <- Arima(gv_ts, order = c(1,2,1), include.drift = FALSE)
summary(fit2)
```

Looking at the error measurements, both the MAE and the RMSE are smaller in the ARIMA(4,2,2) model.

```{r}
naive <- naive(gv_ts,h=5)
mean  <- meanf(gv_ts,h=5)
drift <- rwf(gv_ts,drift=TRUE,h=5)
accuracy(naive)
accuracy(mean)
accuracy(drift)
```

Furthermore, both models beat the benchmark models. Overall, it seems like the ARIMA(4,2,2) model is a better fit.

Now we can finally generate our forecast with the ARIMA(4,2,2) model.

```{r}
# Generate the forecast
forecast_result1 <- forecast(fit1, h = 20)

# Plot the forecast
autoplot(forecast_result1) +
  labs(title = "ARIMA(4,2,2) Forecast",
       x = "Time",
       y = "Real Education Spending per Capita") +
  theme_minimal()
```

This forecast suggests that real education spending per capita will continue to increase over time.

# Conclusion

These analyses imply that that despite the government spending more on education per capita, people believe that education should be prioritized more. One possibility is that government spending on education is being misused and isn't have as big of an effect on the quality of education as it should. Another possibility is that government spending in general hasn't been increasing as much as it should so even if the portion of spending that is being used for education increases, the actual amount being spent isn't meeting the demand.
